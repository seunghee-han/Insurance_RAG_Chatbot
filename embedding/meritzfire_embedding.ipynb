{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88403b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ í˜ì´ì§€: 639\n",
      "ì²« í˜ì´ì§€ ë¯¸ë¦¬ë³´ê¸°:\n",
      " ì¥ê¸° ì•½ê´€_ì œ2025-14í˜¸\n",
      "(ë¬´) 2507\n",
      "ë©”ë¦¬ì¸  ìš´ì „ì ìƒí•´ ì¢…í•©ë³´í—˜\n",
      "íŒë§¤ë²„ì „ 1.0\n",
      "ë³´í—˜ì•½ê´€\n",
      "íŒë§¤ê°œì‹œ 2025. 7. 7\n",
      "â€» ë³¸ ì•½ê´€ì€ ê´€ê³„ ë²•ë ¹ ë° ë‚´ë¶€í†µì œê¸°ì¤€ì— ë”°ë¥¸ ì ˆì°¨ë¥¼ ê±°ì³ ì œê³µë©ë‹ˆë‹¤.\n",
      "ì €ì¥ ì™„ë£Œ: /home/sojung/workspace/skë„¤íŠ¸ì›ìŠ¤ family ai camp/3ì°¨ í”„ë¡œì íŠ¸/pages_raw.json\n"
     ]
    }
   ],
   "source": [
    "# extract_pdf.py\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "PDF_PATH = Path(\"/home/sojung/workspace/skë„¤íŠ¸ì›ìŠ¤ family ai camp/3ì°¨ í”„ë¡œì íŠ¸/6AECH_20250707.pdf\")  # í”„ë¡œì íŠ¸ í´ë”ì˜ íŒŒì¼ëª…ìœ¼ë¡œ ë§ì¶°ì¤˜\n",
    "OUT_JSON = Path(\"/home/sojung/workspace/skë„¤íŠ¸ì›ìŠ¤ family ai camp/3ì°¨ í”„ë¡œì íŠ¸/pages_raw.json\")\n",
    "\n",
    "def extract_text_pages(pdf_path: Path):\n",
    "    text_pages = []\n",
    "    # 1) pdfplumber ìš°ì„  ì‹œë„\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "            for p in pdf.pages:\n",
    "                text_pages.append(p.extract_text() or \"\")\n",
    "    except Exception:\n",
    "        text_pages = []\n",
    "\n",
    "    # 2) PyPDF2 ë³´ì¡°\n",
    "    if not any(s.strip() for s in text_pages):\n",
    "        from PyPDF2 import PdfReader\n",
    "        reader = PdfReader(str(pdf_path))\n",
    "        text_pages = [(p.extract_text() or \"\") for p in reader.pages]\n",
    "\n",
    "    return text_pages\n",
    "\n",
    "def ocr_pdf(pdf_path: Path):\n",
    "    from pdf2image import convert_from_path\n",
    "    import pytesseract\n",
    "    imgs = convert_from_path(str(pdf_path), dpi=300)\n",
    "    texts = []\n",
    "    for im in imgs:\n",
    "        texts.append(pytesseract.image_to_string(im, lang=\"kor+eng\"))\n",
    "    return texts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    assert PDF_PATH.exists(), f\"PDF not found: {PDF_PATH}\"\n",
    "    pages = extract_text_pages(PDF_PATH)\n",
    "\n",
    "    if not any(s.strip() for s in pages):\n",
    "        # ìŠ¤ìº”ë³¸ â†’ OCR\n",
    "        pages = ocr_pdf(PDF_PATH)\n",
    "\n",
    "    print(f\"ì´ í˜ì´ì§€: {len(pages)}\")\n",
    "    print(\"ì²« í˜ì´ì§€ ë¯¸ë¦¬ë³´ê¸°:\\n\", pages[0][:800])\n",
    "\n",
    "    OUT_JSON.write_text(json.dumps(pages, ensure_ascii=False, indent=2))\n",
    "    print(f\"ì €ì¥ ì™„ë£Œ: {OUT_JSON.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23841b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "# !pip -q install \"langchain-postgres>=0.0.12\" \"langchain-openai>=0.2.0\" \"langchain>=0.2.12\" tqdm python-dotenv\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()  # .envì— OPENAI_API_KEYê°€ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n",
    "\n",
    "# ----- ì—°ê²°/ì½œë ‰ì…˜ ì„¤ì • -----\n",
    "CONNECTION_STRING = \"postgresql+psycopg2://play:123@192.168.0.22:5432/team3\"  # ë„¤ê°€ ì“°ë˜ ê°’\n",
    "COLLECTION_NAME   = \"test\"  # ê¸°ì¡´ 'test' ì¬ì‚¬ìš© (í•„ìš”ì‹œ ë³€ê²½)\n",
    "\n",
    "# ----- JSON ê²½ë¡œ ì„¤ì • -----\n",
    "# 1) í”„ë¡œì íŠ¸ ë‚´ ìƒì„±í•œ íŒŒì¼ ê²½ë¡œë¥¼ ë¨¼ì € ì‹œë„\n",
    "CANDIDATES = [\n",
    "    Path(\"/home/sojung/workspace/3ì°¨ ë‹¨ìœ„í”Œì /pages_raw.json\")\n",
    "]\n",
    "\n",
    "JSON_PATH = next((p for p in CANDIDATES if p.exists()), None)\n",
    "assert JSON_PATH is not None, f\"JSON íŒŒì¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í›„ë³´: {[str(p) for p in CANDIDATES]}\"\n",
    "\n",
    "print(\"âœ… JSON ê²½ë¡œ:\", JSON_PATH)\n",
    "print(\"âœ… OPENAI_API_KEY ì¡´ì¬:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbc0268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_pages_as_texts(json_path: Path) -> List[str]:\n",
    "    data = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
    "    # í˜•íƒœ 1: [\"p1 text\", \"p2 text\", ...]\n",
    "    # í˜•íƒœ 2: {\"texts\": [...], \"engines\": [...]}\n",
    "    if isinstance(data, dict) and \"texts\" in data:\n",
    "        texts = data[\"texts\"]\n",
    "    elif isinstance(data, list):\n",
    "        texts = data\n",
    "    else:\n",
    "        raise ValueError(\"ì§€ì›í•˜ì§€ ì•ŠëŠ” JSON í˜•ì‹ì…ë‹ˆë‹¤. ë¦¬ìŠ¤íŠ¸ ë˜ëŠ” {'texts': [...]} ì´ì–´ì•¼ í•©ë‹ˆë‹¤.\")\n",
    "    return [(t or \"\").strip() for t in texts]\n",
    "\n",
    "texts = load_pages_as_texts(JSON_PATH)\n",
    "raw_docs = [\n",
    "    Document(\n",
    "        page_content=txt,\n",
    "        metadata={\n",
    "            \"source\": str(JSON_PATH.resolve()),\n",
    "            \"page\": i+1,\n",
    "            \"doc_name\": JSON_PATH.stem,\n",
    "            \"doc_type\": \"insurance_tos\",\n",
    "        },\n",
    "    )\n",
    "    for i, txt in enumerate(texts) if txt\n",
    "]\n",
    "print(f\"ì´ í˜ì´ì§€: {len(texts)} | ë¹ˆ í˜ì´ì§€ ì œì™¸ ë¬¸ì„œ: {len(raw_docs)}\")\n",
    "if raw_docs:\n",
    "    print(\"ë¯¸ë¦¬ë³´ê¸°:\", raw_docs[0].metadata, raw_docs[0].page_content[:120].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b45bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=512,     # ë¬¸ì„œ ê¸¸ì´ì— ë§ì¶° 800~1500 ì‚¬ì´ ì¡°ì • ê°€ëŠ¥\n",
    "    chunk_overlap=150,\n",
    ")\n",
    "\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "print(f\"ì²­í¬ ìˆ˜: {len(docs)}, í‰ê·  ê¸¸ì´: {sum(len(d.page_content) for d in docs)//max(1,len(docs))}ì\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "\n",
    "# ì´ë¯¸ ì•ì—ì„œ ì„¤ì •í–ˆë‹¤ë©´ ì¬ì‚¬ìš©:\n",
    "# CONNECTION_STRING = \"postgresql+psycopg2://play:123@192.168.0.22:5432/team1\"\n",
    "# COLLECTION_NAME   = \"test\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # 1536ì°¨ì›, ë¹„ìš©/ì†ë„ ê· í˜•\n",
    "\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION_STRING,\n",
    "    use_jsonb=True,           # ë©”íƒ€ë°ì´í„° JSONB ì €ì¥\n",
    "    # recreate_collection=False  # ê¸°ë³¸ê°’; ì ˆëŒ€ ì´ˆê¸°í™” ì•ˆ í•¨\n",
    ")\n",
    "\n",
    "print(\"âœ… PGVector ì¤€ë¹„ ì™„ë£Œ (ê¸°ì¡´ ì»¬ë ‰ì…˜ì— append)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1768cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# NUL(\\x00) í¬í•¨ ì œì–´ë¬¸ì ì œê±° ì •ê·œì‹ (íƒ­/ê°œí–‰ ì œì™¸)\n",
    "CTRL_RE = re.compile(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # 1) NUL ë° ê¸°íƒ€ ì œì–´ë¬¸ì ì œê±°\n",
    "    s = CTRL_RE.sub(\"\", s)\n",
    "    # 2) ìœ ë‹ˆì½”ë“œ ë¹„ì •ìƒ ê³µë°± ì •ê·œí™”(ì„ íƒ)\n",
    "    s = s.replace(\"\\u200b\", \"\").replace(\"\\ufeff\", \"\")\n",
    "    return s\n",
    "\n",
    "def clean_doc(doc: Document) -> Document:\n",
    "    d = deepcopy(doc)\n",
    "    d.page_content = clean_text(d.page_content)\n",
    "    # ë©”íƒ€ë°ì´í„° ê°’ì´ ë¬¸ìì—´ì´ë©´ ë™ì¼ ì²˜ë¦¬\n",
    "    for k, v in list(d.metadata.items()):\n",
    "        if isinstance(v, str):\n",
    "            d.metadata[k] = clean_text(v)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab83eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_docs = []\n",
    "dropped = 0\n",
    "for d in docs:\n",
    "    cd = clean_doc(d)\n",
    "    if cd.page_content.strip():\n",
    "        cleaned_docs.append(cd)\n",
    "    else:\n",
    "        dropped += 1\n",
    "\n",
    "print(f\"ì •ë¦¬ ì™„ë£Œ: {len(cleaned_docs)}ê°œ ìœ ì§€, ë¹ˆ ì²­í¬ {dropped}ê°œ ì œì™¸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q tiktoken\n",
    "\n",
    "import tiktoken\n",
    "from uuid import uuid5, NAMESPACE_URL\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ê³¼ ë§ëŠ” í† í¬ë‚˜ì´ì € (OpenAI ì„ë² ë”©: cl100k_base)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# ëª¨ë¸ í•œê³„: text-embedding-3-small ëŠ” ì…ë ¥ 8191 tokens ê¶Œì¥\n",
    "MAX_TOKENS_PER_ITEM = 8000         # ì•ˆì „ ë§ˆì§„\n",
    "MAX_TOKENS_PER_BATCH = 250_000     # OpenAI ìš”ì²­ë‹¹ 30ë§Œ ì œí•œë³´ë‹¤ ì—¬ìœ ìˆê²Œ\n",
    "MAX_ITEMS_PER_BATCH  = 128         # ê³¼ë„í•œ ë¦¬ìŠ¤íŠ¸ ë°©ì§€\n",
    "\n",
    "def n_tokens(s: str) -> int:\n",
    "    return len(enc.encode(s or \"\"))\n",
    "\n",
    "def truncate_to_tokens(s: str, max_tokens: int = MAX_TOKENS_PER_ITEM) -> str:\n",
    "    toks = enc.encode(s or \"\")\n",
    "    if len(toks) <= max_tokens:\n",
    "        return s\n",
    "    return enc.decode(toks[:max_tokens])\n",
    "\n",
    "def make_id(doc) -> str:\n",
    "    head = (doc.page_content or \"\")[:64]\n",
    "    key = f\"{doc.metadata.get('source')}|{doc.metadata.get('page')}|{head}\"\n",
    "    return str(uuid5(NAMESPACE_URL, key))\n",
    "\n",
    "def build_safe_batches(documents):\n",
    "    \"\"\"ì´ í† í° ìˆ˜/ì•„ì´í…œ ìˆ˜ë¥¼ ì œí•œí•´ ì•ˆì „í•œ ë°°ì¹˜ë¡œ ë¬¶ì–´ì¤Œ.\"\"\"\n",
    "    batch, ids = [], []\n",
    "    tok_sum = 0\n",
    "    for d in documents:\n",
    "        text = truncate_to_tokens(d.page_content)\n",
    "        d.page_content = text  # ì‹¤ì œë¡œ ì˜ë¼ì„œ ì €ì¥\n",
    "\n",
    "        nt = n_tokens(text)\n",
    "        # ë¹„ì–´ìˆëŠ” ì²­í¬ëŠ” ìŠ¤í‚µ\n",
    "        if nt == 0:\n",
    "            continue\n",
    "\n",
    "        # í˜„ì¬ ë°°ì¹˜ì— ì¶”ê°€ ë¶ˆê°€í•˜ë©´ flush\n",
    "        if (tok_sum + nt > MAX_TOKENS_PER_BATCH) or (len(batch) >= MAX_ITEMS_PER_BATCH):\n",
    "            if batch:\n",
    "                yield batch, ids\n",
    "            batch, ids, tok_sum = [], [], 0\n",
    "\n",
    "        batch.append(d)\n",
    "        ids.append(make_id(d))\n",
    "        tok_sum += nt\n",
    "\n",
    "    if batch:\n",
    "        yield batch, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838aa8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total_added = 0\n",
    "num_batches = 0\n",
    "\n",
    "for docs_batch, ids_batch in tqdm(build_safe_batches(cleaned_docs), desc=\"ì„ë² ë”©+ì €ì¥(ë°°ì¹˜)\", unit=\"batch\"):\n",
    "    # ì—¬ê¸°ì„œ vectorstoreê°€ ë°°ì¹˜ ë‹¨ìœ„ë¡œ OpenAI ì„ë² ë”© í˜¸ì¶œ â†’ DB ì €ì¥\n",
    "    vectorstore.add_documents(documents=docs_batch, ids=ids_batch)\n",
    "    total_added += len(docs_batch)\n",
    "    num_batches += 1\n",
    "\n",
    "print(f\"âœ… ì™„ë£Œ: {total_added} ì²­í¬ ì €ì¥, {num_batches}ê°œ ë°°ì¹˜ ì‚¬ìš©\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683cd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 3})\n",
    "query = \"ì²­ì•½ì² íšŒëŠ” ì–¸ì œê¹Œì§€ ê°€ëŠ¥í•œê°€ìš”?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"ğŸ” Query:\", query)\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n[{i}] page={doc.metadata.get('page')}\")\n",
    "    print(doc.page_content[:300].replace(\"\\n\", \" \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insurance-qa310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
