{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88403b40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 페이지: 639\n",
      "첫 페이지 미리보기:\n",
      " 장기 약관_제2025-14호\n",
      "(무) 2507\n",
      "메리츠 운전자 상해 종합보험\n",
      "판매버전 1.0\n",
      "보험약관\n",
      "판매개시 2025. 7. 7\n",
      "※ 본 약관은 관계 법령 및 내부통제기준에 따른 절차를 거쳐 제공됩니다.\n",
      "저장 완료: /home/sojung/workspace/sk네트웍스 family ai camp/3차 프로젝트/pages_raw.json\n"
     ]
    }
   ],
   "source": [
    "# extract_pdf.py\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "PDF_PATH = Path(\"/home/sojung/workspace/sk네트웍스 family ai camp/3차 프로젝트/6AECH_20250707.pdf\")  # 프로젝트 폴더의 파일명으로 맞춰줘\n",
    "OUT_JSON = Path(\"/home/sojung/workspace/sk네트웍스 family ai camp/3차 프로젝트/pages_raw.json\")\n",
    "\n",
    "def extract_text_pages(pdf_path: Path):\n",
    "    text_pages = []\n",
    "    # 1) pdfplumber 우선 시도\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        with pdfplumber.open(str(pdf_path)) as pdf:\n",
    "            for p in pdf.pages:\n",
    "                text_pages.append(p.extract_text() or \"\")\n",
    "    except Exception:\n",
    "        text_pages = []\n",
    "\n",
    "    # 2) PyPDF2 보조\n",
    "    if not any(s.strip() for s in text_pages):\n",
    "        from PyPDF2 import PdfReader\n",
    "        reader = PdfReader(str(pdf_path))\n",
    "        text_pages = [(p.extract_text() or \"\") for p in reader.pages]\n",
    "\n",
    "    return text_pages\n",
    "\n",
    "def ocr_pdf(pdf_path: Path):\n",
    "    from pdf2image import convert_from_path\n",
    "    import pytesseract\n",
    "    imgs = convert_from_path(str(pdf_path), dpi=300)\n",
    "    texts = []\n",
    "    for im in imgs:\n",
    "        texts.append(pytesseract.image_to_string(im, lang=\"kor+eng\"))\n",
    "    return texts\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    assert PDF_PATH.exists(), f\"PDF not found: {PDF_PATH}\"\n",
    "    pages = extract_text_pages(PDF_PATH)\n",
    "\n",
    "    if not any(s.strip() for s in pages):\n",
    "        # 스캔본 → OCR\n",
    "        pages = ocr_pdf(PDF_PATH)\n",
    "\n",
    "    print(f\"총 페이지: {len(pages)}\")\n",
    "    print(\"첫 페이지 미리보기:\\n\", pages[0][:800])\n",
    "\n",
    "    OUT_JSON.write_text(json.dumps(pages, ensure_ascii=False, indent=2))\n",
    "    print(f\"저장 완료: {OUT_JSON.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23841b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필수 라이브러리\n",
    "# !pip -q install \"langchain-postgres>=0.0.12\" \"langchain-openai>=0.2.0\" \"langchain>=0.2.12\" tqdm python-dotenv\n",
    "\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()  # .env에 OPENAI_API_KEY가 있어야 합니다.\n",
    "\n",
    "# ----- 연결/콜렉션 설정 -----\n",
    "CONNECTION_STRING = \"postgresql+psycopg2://play:123@192.168.0.22:5432/team3\"  # 네가 쓰던 값\n",
    "COLLECTION_NAME   = \"test\"  # 기존 'test' 재사용 (필요시 변경)\n",
    "\n",
    "# ----- JSON 경로 설정 -----\n",
    "# 1) 프로젝트 내 생성한 파일 경로를 먼저 시도\n",
    "CANDIDATES = [\n",
    "    Path(\"/home/sojung/workspace/3차 단위플젝/pages_raw.json\")\n",
    "]\n",
    "\n",
    "JSON_PATH = next((p for p in CANDIDATES if p.exists()), None)\n",
    "assert JSON_PATH is not None, f\"JSON 파일을 찾지 못했습니다. 후보: {[str(p) for p in CANDIDATES]}\"\n",
    "\n",
    "print(\"✅ JSON 경로:\", JSON_PATH)\n",
    "print(\"✅ OPENAI_API_KEY 존재:\", bool(os.getenv(\"OPENAI_API_KEY\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbc0268",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def load_pages_as_texts(json_path: Path) -> List[str]:\n",
    "    data = json.loads(json_path.read_text(encoding=\"utf-8\"))\n",
    "    # 형태 1: [\"p1 text\", \"p2 text\", ...]\n",
    "    # 형태 2: {\"texts\": [...], \"engines\": [...]}\n",
    "    if isinstance(data, dict) and \"texts\" in data:\n",
    "        texts = data[\"texts\"]\n",
    "    elif isinstance(data, list):\n",
    "        texts = data\n",
    "    else:\n",
    "        raise ValueError(\"지원하지 않는 JSON 형식입니다. 리스트 또는 {'texts': [...]} 이어야 합니다.\")\n",
    "    return [(t or \"\").strip() for t in texts]\n",
    "\n",
    "texts = load_pages_as_texts(JSON_PATH)\n",
    "raw_docs = [\n",
    "    Document(\n",
    "        page_content=txt,\n",
    "        metadata={\n",
    "            \"source\": str(JSON_PATH.resolve()),\n",
    "            \"page\": i+1,\n",
    "            \"doc_name\": JSON_PATH.stem,\n",
    "            \"doc_type\": \"insurance_tos\",\n",
    "        },\n",
    "    )\n",
    "    for i, txt in enumerate(texts) if txt\n",
    "]\n",
    "print(f\"총 페이지: {len(texts)} | 빈 페이지 제외 문서: {len(raw_docs)}\")\n",
    "if raw_docs:\n",
    "    print(\"미리보기:\", raw_docs[0].metadata, raw_docs[0].page_content[:120].replace(\"\\n\",\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b45bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size=512,     # 문서 길이에 맞춰 800~1500 사이 조정 가능\n",
    "    chunk_overlap=150,\n",
    ")\n",
    "\n",
    "docs = splitter.split_documents(raw_docs)\n",
    "print(f\"청크 수: {len(docs)}, 평균 길이: {sum(len(d.page_content) for d in docs)//max(1,len(docs))}자\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_postgres import PGVector\n",
    "\n",
    "# 이미 앞에서 설정했다면 재사용:\n",
    "# CONNECTION_STRING = \"postgresql+psycopg2://play:123@192.168.0.22:5432/team1\"\n",
    "# COLLECTION_NAME   = \"test\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")  # 1536차원, 비용/속도 균형\n",
    "\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    connection=CONNECTION_STRING,\n",
    "    use_jsonb=True,           # 메타데이터 JSONB 저장\n",
    "    # recreate_collection=False  # 기본값; 절대 초기화 안 함\n",
    ")\n",
    "\n",
    "print(\"✅ PGVector 준비 완료 (기존 컬렉션에 append)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1768cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from copy import deepcopy\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# NUL(\\x00) 포함 제어문자 제거 정규식 (탭/개행 제외)\n",
    "CTRL_RE = re.compile(r\"[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    # 1) NUL 및 기타 제어문자 제거\n",
    "    s = CTRL_RE.sub(\"\", s)\n",
    "    # 2) 유니코드 비정상 공백 정규화(선택)\n",
    "    s = s.replace(\"\\u200b\", \"\").replace(\"\\ufeff\", \"\")\n",
    "    return s\n",
    "\n",
    "def clean_doc(doc: Document) -> Document:\n",
    "    d = deepcopy(doc)\n",
    "    d.page_content = clean_text(d.page_content)\n",
    "    # 메타데이터 값이 문자열이면 동일 처리\n",
    "    for k, v in list(d.metadata.items()):\n",
    "        if isinstance(v, str):\n",
    "            d.metadata[k] = clean_text(v)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab83eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_docs = []\n",
    "dropped = 0\n",
    "for d in docs:\n",
    "    cd = clean_doc(d)\n",
    "    if cd.page_content.strip():\n",
    "        cleaned_docs.append(cd)\n",
    "    else:\n",
    "        dropped += 1\n",
    "\n",
    "print(f\"정리 완료: {len(cleaned_docs)}개 유지, 빈 청크 {dropped}개 제외\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38f7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -q tiktoken\n",
    "\n",
    "import tiktoken\n",
    "from uuid import uuid5, NAMESPACE_URL\n",
    "\n",
    "# 임베딩 모델과 맞는 토크나이저 (OpenAI 임베딩: cl100k_base)\n",
    "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# 모델 한계: text-embedding-3-small 는 입력 8191 tokens 권장\n",
    "MAX_TOKENS_PER_ITEM = 8000         # 안전 마진\n",
    "MAX_TOKENS_PER_BATCH = 250_000     # OpenAI 요청당 30만 제한보다 여유있게\n",
    "MAX_ITEMS_PER_BATCH  = 128         # 과도한 리스트 방지\n",
    "\n",
    "def n_tokens(s: str) -> int:\n",
    "    return len(enc.encode(s or \"\"))\n",
    "\n",
    "def truncate_to_tokens(s: str, max_tokens: int = MAX_TOKENS_PER_ITEM) -> str:\n",
    "    toks = enc.encode(s or \"\")\n",
    "    if len(toks) <= max_tokens:\n",
    "        return s\n",
    "    return enc.decode(toks[:max_tokens])\n",
    "\n",
    "def make_id(doc) -> str:\n",
    "    head = (doc.page_content or \"\")[:64]\n",
    "    key = f\"{doc.metadata.get('source')}|{doc.metadata.get('page')}|{head}\"\n",
    "    return str(uuid5(NAMESPACE_URL, key))\n",
    "\n",
    "def build_safe_batches(documents):\n",
    "    \"\"\"총 토큰 수/아이템 수를 제한해 안전한 배치로 묶어줌.\"\"\"\n",
    "    batch, ids = [], []\n",
    "    tok_sum = 0\n",
    "    for d in documents:\n",
    "        text = truncate_to_tokens(d.page_content)\n",
    "        d.page_content = text  # 실제로 잘라서 저장\n",
    "\n",
    "        nt = n_tokens(text)\n",
    "        # 비어있는 청크는 스킵\n",
    "        if nt == 0:\n",
    "            continue\n",
    "\n",
    "        # 현재 배치에 추가 불가하면 flush\n",
    "        if (tok_sum + nt > MAX_TOKENS_PER_BATCH) or (len(batch) >= MAX_ITEMS_PER_BATCH):\n",
    "            if batch:\n",
    "                yield batch, ids\n",
    "            batch, ids, tok_sum = [], [], 0\n",
    "\n",
    "        batch.append(d)\n",
    "        ids.append(make_id(d))\n",
    "        tok_sum += nt\n",
    "\n",
    "    if batch:\n",
    "        yield batch, ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "838aa8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "total_added = 0\n",
    "num_batches = 0\n",
    "\n",
    "for docs_batch, ids_batch in tqdm(build_safe_batches(cleaned_docs), desc=\"임베딩+저장(배치)\", unit=\"batch\"):\n",
    "    # 여기서 vectorstore가 배치 단위로 OpenAI 임베딩 호출 → DB 저장\n",
    "    vectorstore.add_documents(documents=docs_batch, ids=ids_batch)\n",
    "    total_added += len(docs_batch)\n",
    "    num_batches += 1\n",
    "\n",
    "print(f\"✅ 완료: {total_added} 청크 저장, {num_batches}개 배치 사용\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683cd317",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 3})\n",
    "query = \"청약철회는 언제까지 가능한가요?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"🔎 Query:\", query)\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n[{i}] page={doc.metadata.get('page')}\")\n",
    "    print(doc.page_content[:300].replace(\"\\n\", \" \"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "insurance-qa310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
